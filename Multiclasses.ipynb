{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87bbb778",
   "metadata": {},
   "source": [
    "# Exercice 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6bd8df",
   "metadata": {},
   "source": [
    "-**Que signifie concrètement le théorème d'approximation universelle ?**  \n",
    "\n",
    "    Ce théorème stipule qu'un réseau de neurones avec une seule couche cachée et un nombre suffisant de neurones peut approximer n'importe quelle fonction continue sur un espace compact. Cela signifie que, en théorie, un tel réseau est capable de modéliser des relations complexes entre les entrées et les sorties, ce qui le rend très puissant pour une variété de tâches d'apprentissage automatique.\n",
    "\n",
    "-**Ce théorème garantit-il qu'on peut toujours trouver les bons poids ?**  \n",
    "\n",
    "    Non, le théorème d'approximation universelle ne garantit pas que les poids du réseau de neurones peuvent être trouvés pour une fonction donnée. Il indique seulement qu'un réseau de neurones avec une architecture appropriée peut approximer la fonction.\n",
    "\n",
    "-**Quelle est la différence entre \"pouvoir approximer\" et \"pouvoir apprendre\" ?**  \n",
    "\n",
    "    \"Pouvoir approximer\" signifie qu'un modèle peut représenter une fonction ou une relation entre les entrées et les sorties. \"Pouvoir apprendre\" signifie que le modèle peut ajuster ses paramètres (poids) à partir des données d'entraînement pour minimiser l'erreur entre les prédictions du modèle et les valeurs réelles. Un modèle peut être capable d'approximer une fonction sans être capable de l'apprendre efficacement à partir des données.\n",
    "\n",
    "-**Pourquoi utilise-t-on souvent beaucoup plus de couches cachées en pratique ?**  \n",
    "\n",
    "    En pratique, on utilise souvent beaucoup plus de couches cachées car les fonctions complexes que nous voulons modéliser peuvent nécessiter des représentations hiérarchiques. Les couches supplémentaires permettent au réseau de capturer des caractéristiques de plus en plus abstraites des données, ce qui améliore la capacité du modèle à généraliser et à faire des prédictions précises sur de nouvelles données.\n",
    "\n",
    "-**En principe, vous avez déjà vu au lycée un autre type d’approximateur de fonctions, donner leurs noms ?**\n",
    "\n",
    "    Les polynômes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb1ed4b",
   "metadata": {},
   "source": [
    "### Expliquer la phrase suivante : Le théorème d’approximation universelle affirme qu’un réseau profond peut exactement retrouver les données d’entraînement.\n",
    "\n",
    "        Le théorème d'approximation universelle affirme qu'un réseau profond, c'est-à-dire un réseau de neurones avec plusieurs couches cachées, peut approximer n'importe quelle fonction continue sur un espace compact. Cela signifie que, si le réseau est suffisamment profond et possède un nombre adéquat de neurones, il peut apprendre à reproduire exactement les données d'entraînement. En d'autres termes, le réseau peut ajuster ses poids pour minimiser l'erreur entre ses prédictions et les valeurs réelles des données d'entraînement, ce qui lui permet de \"retrouver\" ces données avec une précision arbitraire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9024c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class ActivationFunction:\n",
    "    def __init__(self, name, alpha=0.01):\n",
    "        self.name = name.lower()\n",
    "        self.alpha = alpha  # Pour Leaky ReLU\n",
    "\n",
    "    def apply(self, z):\n",
    "        if self.name == \"heaviside\":\n",
    "            return np.where(z >= 0, 1, 0)\n",
    "        elif self.name == \"sigmoid\":\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "        elif self.name == \"tanh\":\n",
    "            return np.tanh(z)\n",
    "        elif self.name == \"relu\":\n",
    "            return np.where(z >= 0, z, 0)\n",
    "        elif self.name == \"leaky_relu\":\n",
    "            return np.where(z >= 0, z, self.alpha * z)\n",
    "        else:\n",
    "            raise ValueError(f\"Activation '{self.name}' non reconnue.\")\n",
    "\n",
    "    def derivative(self, z):\n",
    "        if self.name == \"heaviside\":\n",
    "            return np.zeros_like(z)\n",
    "        elif self.name == \"sigmoid\":\n",
    "            sig = self.apply(z)\n",
    "            return sig * (1 - sig)\n",
    "        elif self.name == \"tanh\":\n",
    "            tanh_z = self.apply(z)\n",
    "            return 1 - tanh_z ** 2\n",
    "        elif self.name == \"relu\":\n",
    "            return np.where(z > 0, 1, 0)\n",
    "        elif self.name == \"leaky_relu\":\n",
    "            return np.where(z > 0, 1, self.alpha)\n",
    "        else:\n",
    "            raise ValueError(f\"Dérivée de '{self.name}' non définie.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9612c167",
   "metadata": {},
   "source": [
    "# Exercice 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43abe815",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoucheNeurones:\n",
    "    def __init__(self, n_input, n_neurons, activation='sigmoid', learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Initialise une couche de neurones\n",
    "\n",
    "        Parameters:\n",
    "        - n_input: nombre d'entrées\n",
    "        - n_neurons: nombre de neurones dans cette couche\n",
    "        - activation: fonction d'activation ('sigmoid', 'tanh', 'relu')\n",
    "        - learning_rate: taux d'apprentissage\n",
    "        \"\"\"\n",
    "        self.n_input = n_input\n",
    "        self.n_neurons = n_neurons\n",
    "        self.activation_name = activation\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Initialisation Xavier/Glorot pour éviter l'explosion/disparition des gradients\n",
    "        limit = np.sqrt(6 / (n_input + n_neurons))\n",
    "        self.weights = np.random.uniform(-limit, limit, (n_neurons, n_input))\n",
    "        self.bias = np.zeros((n_neurons, 1))\n",
    "\n",
    "        # Variables pour stocker les valeurs lors de la propagation\n",
    "        self.last_input = None\n",
    "        self.last_z = None\n",
    "        self.last_activation = None\n",
    "\n",
    "        # Import de la fonction d'activation du TP précédent\n",
    "        self.activation_func = ActivationFunction(activation)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Propagation avant\n",
    "        X: matrice d'entrée (n_features, n_samples)\n",
    "        \"\"\"\n",
    "        self.last_input = X # Stockage de l'entrée pour la rétropropagation\n",
    "        self.last_z = np.dot(self.weights, X) + self.bias # combinaison linéaire\n",
    "        # Application de la fonction d'activation\n",
    "        self.last_activation = self.activation_func.apply(self.last_z) #application de sigmoid\n",
    "        return self.last_activation\n",
    "\n",
    "    def backward(self, gradient_from_next_layer):\n",
    "        \"\"\"\n",
    "        Rétropropagation\n",
    "        gradient_from_next_layer: gradient venant de la couche suivante\n",
    "        \"\"\"\n",
    "        # TODO: Calculer les gradients par rapport aux poids et biais\n",
    "        # TODO: Calculer le gradient à propager vers la couche précédente\n",
    "\n",
    "        # Gradient par rapport à la fonction d'activation\n",
    "        grad_activation = 0\n",
    "\n",
    "        # Gradient par rapport aux poids\n",
    "        grad_weights = 0\n",
    "\n",
    "        # Gradient par rapport aux biais  \n",
    "        grad_bias = 0\n",
    "\n",
    "        # Gradient à propager vers la couche précédente\n",
    "        grad_input = 0\n",
    "\n",
    "        # Mise à jour des paramètres\n",
    "        self.weights -= self.learning_rate * grad_weights\n",
    "        self.bias -= self.learning_rate * grad_bias\n",
    "\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c35a894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sortie de la couche (forward):\n",
      "[[0.24215282]\n",
      " [0.46212341]\n",
      " [0.52594212]]\n"
     ]
    }
   ],
   "source": [
    "# Test unitaire de la méthode forward\n",
    "# Création d'une couche avec 2 entrées et 3 neurones\n",
    "couche = CoucheNeurones(n_input=2, n_neurons=3, activation='sigmoid')\n",
    "# Entrée de test : 2 features, 1 échantillon\n",
    "X_test = np.array([[0.5], [0.8]])\n",
    "# Passage avant\n",
    "output = couche.forward(X_test)\n",
    "print(\"Sortie de la couche (forward):\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edb67287",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronMultiCouches:\n",
    "    def __init__(self, architecture, learning_rate=0.01, activation='sigmoid'):\n",
    "        \"\"\"\n",
    "        architecture: liste des tailles de couches [input_size, hidden1, hidden2, ..., output_size]\n",
    "        \"\"\"\n",
    "        self.architecture = architecture\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activation = activation\n",
    "        self.couches = []\n",
    "        self.history = {'loss': [], 'val_loss': [], 'accuracy': [], 'val_accuracy': []}\n",
    "\n",
    "        # Création des couches\n",
    "        for i in range(len(architecture) - 1):\n",
    "            # TODO: Créer les couches successives\n",
    "            # La dernière couche peut avoir une activation différente\n",
    "            activation_couche = activation\n",
    "            if i == len(architecture) - 2:  # Dernière couche\n",
    "                activation_couche = 'sigmoid'  # ou 'softmax' pour multi-classes\n",
    "\n",
    "            couche = CoucheNeurones(\n",
    "                n_input=architecture[i],\n",
    "                n_neurons=architecture[i+1], \n",
    "                activation=activation_couche,\n",
    "                learning_rate=learning_rate\n",
    "            )\n",
    "            self.couches.append(couche)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Propagation avant à travers tout le réseau\n",
    "        \"\"\"\n",
    "        current_input = X.T  # Transposer pour avoir (n_features, n_samples)\n",
    "        # Propager les données à travers toutes les couches\n",
    "        for couche in self.couches:\n",
    "            current_input = couche.forward(current_input)\n",
    "        # Retourner la sortie finale\n",
    "        return current_input.T  # Retransposer pour avoir (n_samples, n_output)\n",
    "\n",
    "    def backward(self, X, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Rétropropagation à travers tout le réseau\n",
    "        \"\"\"\n",
    "        # TODO: Calculer le gradient initial (dérivée de la fonction de coût)\n",
    "        # Pour l'erreur quadratique : gradient = (y_pred - y_true)\n",
    "        # TODO: Propager le gradient vers l'arrière\n",
    "\n",
    "    def train_epoch(self, X, y):\n",
    "        \"\"\"\n",
    "        Une époque d'entraînement\n",
    "        \"\"\"\n",
    "        # Propagation avant\n",
    "        y_pred = self.forward(X)\n",
    "\n",
    "        # Calcul de la fonction de perte\n",
    "        loss = self.compute_loss(y, y_pred)\n",
    "\n",
    "        # Rétropropagation\n",
    "        self.backward(X, y, y_pred)\n",
    "\n",
    "        return loss, y_pred\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calcule la fonction de coût (erreur quadratique moyenne)\n",
    "        \"\"\"\n",
    "        # TODO: Implémenter l'erreur quadratique moyenne\n",
    "        return 0\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None, epochs=100, verbose=True):\n",
    "        \"\"\"\n",
    "        Entraîne le réseau\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            # Entraînement\n",
    "            loss, y_pred = self.train_epoch(X, y)\n",
    "            accuracy = self.compute_accuracy(y, y_pred)\n",
    "\n",
    "            self.history['loss'].append(loss)\n",
    "            self.history['accuracy'].append(accuracy)\n",
    "\n",
    "            # Validation si données fournies\n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_val_pred = self.predict(X_val)\n",
    "                val_loss = self.compute_loss(y_val, y_val_pred)\n",
    "                val_accuracy = self.compute_accuracy(y_val, y_val_pred)\n",
    "\n",
    "                self.history['val_loss'].append(val_loss)\n",
    "                self.history['val_accuracy'].append(val_accuracy)\n",
    "\n",
    "            if verbose and epoch % 10 == 0:\n",
    "                print(f\"Époque {epoch:3d} - Loss: {loss:.4f} - Acc: {accuracy:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Prédiction sur de nouvelles données\n",
    "        \"\"\"\n",
    "        return self.forward(X)\n",
    "\n",
    "    def compute_accuracy(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calcule l'accuracy pour la classification binaire\n",
    "        \"\"\"\n",
    "        # TODO: Implémenter le calcul d'accuracy\n",
    "        # Pour la classification binaire : seuil à 0.5\n",
    "        predictions = (y_pred > 0.5).astype(int)\n",
    "        return np.mean(predictions.flatten() == y_true.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fa62c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test forward passé avec succès.\n",
      "Forme de l'entrée: (1, 2)\n",
      "Forme de la sortie: (1, 3)\n",
      "Sortie: [[0.24215282 0.46212341 0.52594212]]\n"
     ]
    }
   ],
   "source": [
    "# Test unitaire de la méthode forward du perceptron multi-couches\n",
    "\n",
    "# Création d'un perceptron avec l'architecture correspondante à notre couche test\n",
    "# La couche a 2 entrées et 3 neurones, donc l'architecture doit être [2, 3]\n",
    "perceptron = PerceptronMultiCouches([2, 3])\n",
    "\n",
    "# Remplacement des couches par notre couche de test pour contrôler la sortie\n",
    "perceptron.couches = [couche]\n",
    "\n",
    "# Test avec X_test - on transpose X_test pour avoir la forme (1, 2) comme attendu par le perceptron\n",
    "X_test_reshaped = X_test.T  # Transformation de (2,1) vers (1,2)\n",
    "result = perceptron.forward(X_test_reshaped)\n",
    "\n",
    "# La sortie attendue doit aussi être transposée pour la comparaison\n",
    "output_expected = output.T  # Transformation pour correspondre au format de sortie du perceptron\n",
    "\n",
    "assert np.allclose(result, output_expected), f\"Résultat attendu: {output_expected}, obtenu: {result}\"\n",
    "print(\"Test forward passé avec succès.\")\n",
    "print(f\"Forme de l'entrée: {X_test_reshaped.shape}\")\n",
    "print(f\"Forme de la sortie: {result.shape}\")\n",
    "print(f\"Sortie: {result}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be616768",
   "metadata": {},
   "source": [
    "# Exercice 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "837ccd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sur le problème XOR\n",
      "Données d'entrée :\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "Sorties attendues :\n",
      "[0 1 1 0]\n",
      "\n",
      "--- Architecture [2, 2, 1] ---\n",
      "Prédictions :\n",
      "  [0 0] -> 0.4043 (attendu: 0)\n",
      "  [0 1] -> 0.3463 (attendu: 1)\n",
      "  [1 0] -> 0.3534 (attendu: 1)\n",
      "  [1 1] -> 0.3136 (attendu: 0)\n",
      "Accuracy finale : 0.5000\n",
      "\n",
      "--- Architecture [2, 3, 1] ---\n",
      "Prédictions :\n",
      "  [0 0] -> 0.3420 (attendu: 0)\n",
      "  [0 1] -> 0.2991 (attendu: 1)\n",
      "  [1 0] -> 0.3461 (attendu: 1)\n",
      "  [1 1] -> 0.3070 (attendu: 0)\n",
      "Accuracy finale : 0.5000\n",
      "\n",
      "--- Architecture [2, 4, 1] ---\n",
      "Prédictions :\n",
      "  [0 0] -> 0.3428 (attendu: 0)\n",
      "  [0 1] -> 0.3428 (attendu: 1)\n",
      "  [1 0] -> 0.3128 (attendu: 1)\n",
      "  [1 1] -> 0.3130 (attendu: 0)\n",
      "Accuracy finale : 0.5000\n",
      "\n",
      "--- Architecture [2, 2, 2, 1] ---\n",
      "Prédictions :\n",
      "  [0 0] -> 0.3994 (attendu: 0)\n",
      "  [0 1] -> 0.3930 (attendu: 1)\n",
      "  [1 0] -> 0.4092 (attendu: 1)\n",
      "  [1 1] -> 0.4025 (attendu: 0)\n",
      "Accuracy finale : 0.5000\n"
     ]
    }
   ],
   "source": [
    "def test_xor():\n",
    "    \"\"\"\n",
    "    Test du réseau multicouches sur le problème XOR\n",
    "    \"\"\"\n",
    "    # Données XOR\n",
    "    X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "    y_xor = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "    print(\"Test sur le problème XOR\")\n",
    "    print(\"Données d'entrée :\")\n",
    "    print(X_xor)\n",
    "    print(\"Sorties attendues :\")\n",
    "    print(y_xor.flatten())\n",
    "\n",
    "    # TODO: Créer et entraîner le réseau\n",
    "    # Essayez différentes architectures\n",
    "    architectures = [\n",
    "        [2, 2, 1],    # 2 entrées, 2 neurones cachés, 1 sortie\n",
    "        [2, 3, 1],    # 2 entrées, 3 neurones cachés, 1 sortie  \n",
    "        [2, 4, 1],    # 2 entrées, 4 neurones cachés, 1 sortie\n",
    "        [2, 2, 2, 1], # 2 couches cachées\n",
    "    ]\n",
    "\n",
    "    for arch in architectures:\n",
    "        print(f\"\\n--- Architecture {arch} ---\")\n",
    "\n",
    "        # Créer et entraîner le réseau\n",
    "        mlp = PerceptronMultiCouches(arch, learning_rate=0.5, activation='sigmoid')\n",
    "        mlp.fit(X_xor, y_xor, epochs=1000, verbose=False)\n",
    "\n",
    "        # Test des prédictions\n",
    "        predictions = mlp.predict(X_xor)\n",
    "        print(\"Prédictions :\")\n",
    "        for i in range(len(X_xor)):\n",
    "            print(f\"  {X_xor[i]} -> {predictions[i][0]:.4f} (attendu: {y_xor[i][0]})\")\n",
    "\n",
    "        # Calculer l'accuracy\n",
    "        accuracy = mlp.compute_accuracy(y_xor, predictions)\n",
    "        print(f\"Accuracy finale : {accuracy:.4f}\")\n",
    "\n",
    "# Lancer le test\n",
    "test_xor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea29e98",
   "metadata": {},
   "source": [
    "-**Le réseau arrive-t-il à résoudre XOR ? Avec quelle architecture minimale ?**  \n",
    "\n",
    "        Non ça n'arrive pas car nous n'avons pas implémenter la rétropropagation. Sans cela le réseau ne peut pas apprendre les poids nécessaires pour résoudre le problème XOR. L'architecture minimale pour résoudre le problème XOR est un réseau avec au moins une couche cachée contenant deux neurones, d'après se que j'ai lu (J'ai appliqué la consigne donc pas de backward).\n",
    "\n",
    "-**Comment le nombre de neurones cachés influence-t-il la convergence ?**  \n",
    "\n",
    "        Si on a pas assez de neurones cachés, le réseau peut ne pas être capable de capturer la complexité des données, ce qui peut entraîner une mauvaise convergence ou un sur-apprentissage. En revanche, si on a trop de neurones cachés, le réseau peut devenir trop complexe et surajuster les données d'entraînement, ce qui peut également nuire à la convergence. Il faut donc un juste milieu.\n",
    "\n",
    "-**Que se passe-t-il avec plusieurs couches cachées ?**  \n",
    "\n",
    "        Avec plusieurs couches cachées, le réseau devient plus profond et peut capturer des relations plus complexes dans les données.\n",
    "\n",
    "-**L'initialisation des poids a-t-elle une influence ? (tester d'autres types d'initialisations)**  \n",
    "\n",
    "        Oui, l'initialisation des poids a une influence significative sur la convergence du réseau. Une mauvaise initialisation peut entraîner des problèmes tels que la saturation des fonctions d'activation, ce qui rend difficile l'apprentissage. Des initialisations comme Xavier (utillisé ici) sont souvent utilisées pour améliorer la convergence en tenant compte de la taille des couches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
